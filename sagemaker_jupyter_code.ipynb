{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Convolution1D, GlobalMaxPool1D, MaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"embeddings_dictionary_size\": 500000,\n",
    "    \"embeddings_vector_size\": 50,\n",
    "    \"padding_size\": 20,\n",
    "    \"batch_size\": 1000,\n",
    "    \"embeddings_path\": \"glove.50d.txt\",\n",
    "    \"input_tensor_name\": \"embedding_input\",\n",
    "    \"num_epoch\":5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to read data from S3\n",
    "def read_data(path,mode):\n",
    "           \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    path_split = path.replace(\"s3://\", \"\").split(\"/\")\n",
    "\n",
    "    bucket = path_split.pop(0)\n",
    "    key = \"/\".join(path_split)\n",
    "\n",
    "    data = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "    decoded_file = data[\"Body\"].read().decode('utf-8').split('\\n')\n",
    "    \n",
    "    features=[]; sentiment=[]\n",
    "    for line in decoded_file:\n",
    "        content = json.loads(line)\n",
    "        features.append(content['features'])\n",
    "        sentiment.append(content[\"sentiment\"]/4)  \n",
    "    \n",
    "    num_data_points = len(features)\n",
    "    num_batches = math.ceil(len(features)/config['batch_size'])\n",
    "    \n",
    "    Dataset = tf.data.Dataset\n",
    "    \n",
    "    dataset = Dataset.from_tensor_slices((features, sentiment))\n",
    "\n",
    "    if mode == \"train\":\n",
    "\n",
    "        dataset = Dataset.from_tensor_slices((features, sentiment))\n",
    "        dataset = dataset.batch(config[\"batch_size\"]).shuffle(10000, seed=12345).repeat(\n",
    "            config[\"num_epoch\"])\n",
    "        num_batches = math.ceil(len(features) / config[\"batch_size\"])\n",
    "\n",
    "    if mode in (\"validation\", \"eval\"):\n",
    "\n",
    "        dataset = dataset.batch(config[\"batch_size\"]).repeat(config[\"num_epoch\"])\n",
    "        num_batches = int(math.ceil(len(features) / config[\"batch_size\"]))\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    dataset_features, dataset_sentiments = iterator.get_next()\n",
    "\n",
    "\n",
    "    return [{config[\"input_tensor_name\"]: dataset_features}, dataset_sentiments,\n",
    "            {\"num_data_point\": num_data_points, \"num_batches\": num_batches}]\n",
    "\n",
    "#read data from S3\n",
    "train_dataset=read_data('s3://ai-assignment/assignment6/data/train.json','train')\n",
    "eval_dataset=read_data('s3://ai-assignment/assignment6/data/eval.json','eval')\n",
    "dev_dataset=read_data('s3://ai-assignment/assignment6/data/dev.json','validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to read embedding dictionary with embeddings_path\n",
    "def read_dictionary(path,embeddings_dictionary_size,embeddings_vector_size):\n",
    "    \n",
    "    embedding_matrix = np.zeros((embeddings_dictionary_size, embeddings_vector_size))\n",
    "    \n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    path_split = path.replace(\"s3://\", \"\").split(\"/\")\n",
    "\n",
    "    bucket = path_split.pop(0)\n",
    "    key = \"/\".join(path_split)\n",
    "\n",
    "    data = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "    decoded_file = data[\"Body\"].read().decode('utf-8').split('\\n')\n",
    "    \n",
    "    for i in range(embeddings_dictionary_size):\n",
    "        if len(decoded_file[i].split()[1:]) != embeddings_vector_size:\n",
    "            continue\n",
    "        embedding_matrix[i] = np.asarray(decoded_file[i].split()[1:], dtype='float32')\n",
    "           \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN model\n",
    "def keras_model_fn(_, config):\n",
    "    \"\"\"\n",
    "    Creating a CNN model for sentiment modeling\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_matrix = read_dictionary('s3://ai-assignment/assignment6/glove.50d.txt',config[\"embeddings_dictionary_size\"],config[\"embeddings_vector_size\"])\n",
    "\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(weights=[embedding_matrix], input_length = config[\"padding_size\"],input_dim = config[\"embeddings_dictionary_size\"],output_dim = config[\"embeddings_vector_size\"], trainable = True))\n",
    "    cnn_model.add(Convolution1D(filters=200,kernel_size=3,strides = 1, padding='valid',activation = 'relu'))\n",
    "    cnn_model.add(MaxPool1D(pool_size = 2))\n",
    "    cnn_model.add(Convolution1D(filters=100,kernel_size=2,strides = 1, padding='valid',activation = 'relu'))\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    cnn_model.add(Dense(units=100, activation = 'relu'))\n",
    "    cnn_model.add(Dense(units=1, activation = 'sigmoid'))\n",
    "    Adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    cnn_model.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics =['accuracy'])\n",
    "\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define main function to fit model\n",
    "def main(train_dataset,validation_dataset,eval_dataset):\n",
    "    \"\"\"\n",
    "    Main training method\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preparing for training...\")\n",
    "\n",
    "    training_config = config    \n",
    "\n",
    "    model = keras_model_fn(None, training_config)\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    model.fit(\n",
    "        x=train_dataset[0]['embedding_input'], y=train_dataset[1], steps_per_epoch=train_dataset[2][\"num_batches\"],\n",
    "        epochs=training_config[\"num_epoch\"],\n",
    "        validation_data=(validation_dataset[0]['embedding_input'], validation_dataset[1]),\n",
    "        validation_steps=validation_dataset[2][\"num_batches\"])\n",
    "\n",
    "    score = model.evaluate(\n",
    "        eval_dataset[0]['embedding_input'], eval_dataset[1], steps=eval_dataset[2][\"num_batches\"], verbose=0)\n",
    "\n",
    "    print(\"Test loss:{}\".format(score[0]))\n",
    "    print(\"Test accuracy:{}\".format(score[1]))\n",
    "    \n",
    "\n",
    "    # save model\n",
    "    str_ = tf.contrib.saved_model.save_keras_model(model, \"assignment6/output3/sentiment_model.h5\")\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    s3.put_object(Bucket='ai-assignment',Key=('sentiment_model.h5/3/assets'+'/'))\n",
    "    s3.put_object(Bucket='ai-assignment',Key=('sentiment_model.h5/3/variables'+'/'))\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    s3.meta.client.upload_file(str_.decode() +'/saved_model.pb', 'ai-assignment', 'assignment6/output/sentiment_model.h5/3/saved_model.pb')\n",
    "    s3.meta.client.upload_file(str_.decode() +'/variables/variables.data-00000-of-00001', 'ai-assignment', 'assignment6/output/sentiment_model.h5/3/variables/variables.data-00000-of-00001')\n",
    "    s3.meta.client.upload_file(str_.decode() +'/variables/variables.index', 'ai-assignment', 'assignment6/output/sentiment_model.h5/3/variables/variables.index')\n",
    "    s3.meta.client.upload_file(str_.decode() +'/assets/saved_model.json', 'ai-assignment', 'assignment6/output/sentiment_model.h5/3/assets/saved_model.json')\n",
    "    s3.meta.client.upload_file(str_.decode() +'/variables/checkpoint', 'ai-assignment', 'assignment6/output/sentiment_model.h5/3/variables/checkpoint')\n",
    "    \n",
    "    print('Model successfully saved')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for training...\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Starting training...\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/5\n",
      "1360/1360 [==============================] - 2996s 2s/step - loss: 0.4553 - acc: 0.7812 - val_loss: 0.4247 - val_acc: 0.8025\n",
      "Epoch 2/5\n",
      "1360/1360 [==============================] - 2979s 2s/step - loss: 0.4083 - acc: 0.8112 - val_loss: 0.4131 - val_acc: 0.8090\n",
      "Epoch 3/5\n",
      "1360/1360 [==============================] - 2906s 2s/step - loss: 0.3901 - acc: 0.8219 - val_loss: 0.4120 - val_acc: 0.8091\n",
      "Epoch 4/5\n",
      "1360/1360 [==============================] - 2897s 2s/step - loss: 0.3750 - acc: 0.8301 - val_loss: 0.4078 - val_acc: 0.8125\n",
      "Epoch 5/5\n",
      "1360/1360 [==============================] - 2932s 2s/step - loss: 0.3619 - acc: 0.8375 - val_loss: 0.4105 - val_acc: 0.8130\n",
      "Test loss:0.4115882899612188\n",
      "Test accuracy:0.8127375245094299\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fd1405fdb70>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "WARNING:tensorflow:Model was compiled with an optimizer, but the optimizer is not from `tf.train` (e.g. `tf.train.AdagradOptimizer`). Only the serving graph was exported. The train and evaluate graphs were not added to the SavedModel.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: assignment6/output3/sentiment_model.h5/1583726472/saved_model.pb\n",
      "Model successfully saved\n"
     ]
    }
   ],
   "source": [
    "model = main(train_dataset,dev_dataset,eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
